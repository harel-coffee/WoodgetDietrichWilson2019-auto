{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run TPOT\n",
    "Runs the TPOT genetic algorithm searching method to try and find the best classifer to use. Then investigate the\n",
    "results manually to try and get the best trade-off between classifier complexity and accuracy - look along the 'Pareto front' and pick the best manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ErrorML.ErrorML import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tpot import TPOTClassifier, TPOTRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def robin_metric(y_true, y_pred):\n",
    "    \"\"\"A metric of accuracy that ignores the class with the highest accuracy.\n",
    "    \n",
    "    Given y_true and y_pred, it calculates a confusion matrix, and then takes\n",
    "    the average of the diagonal elements of the matrix, ignoring the highest value.\n",
    "    This gives us an accuracy of all but the class which is predicted best - which\n",
    "    is useful for imbalanced learning, where one class is always predicted very well.\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Normalize confusion matrix\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    metric = np.sort(np.diag(cm))[:-1].mean()\n",
    "    \n",
    "    return metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data('2017_ValidationPts_ALL_Update15March19b_ROBIN.csv')\n",
    "X, y = get_processed_data(df, classes=[-2, -0.2, 0.2, 6.5], categorised=True, focal=False,\n",
    "                          scale=False, exclude=None, absolute=False, subset=None)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robin/anaconda3/envs/amy/lib/python3.7/site-packages/tpot/base.py:309: DeprecationWarning: Scoring function <function robin_metric at 0x11ce5fea0> looks like it is a metric function rather than a scikit-learn scorer. This scoring type was deprecated in version TPOT 0.9.1 and will be removed in version 0.11. Please update your custom scoring function.\n",
      "  'Please update your custom scoring function.'.format(scoring), DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 operators have been imported by TPOT.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff1cfa866e4448a3a05d6a6c366d9e38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Optimization Progress', max=10100, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 69\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 1 - Current Pareto front scores:\n",
      "-1\t0.594735760383695\tBernoulliNB(input_matrix, BernoulliNB__alpha=0.1, BernoulliNB__fit_prior=False)\n",
      "-2\t0.6820957606499649\tGaussianNB(XGBClassifier(input_matrix, XGBClassifier__learning_rate=0.001, XGBClassifier__max_depth=10, XGBClassifier__min_child_weight=4, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.7500000000000001))\n",
      "-3\t0.6914488245847021\tGaussianNB(PolynomialFeatures(XGBClassifier(input_matrix, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=2, XGBClassifier__min_child_weight=2, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.35000000000000003), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 93\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 71\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "Generation 2 - Current Pareto front scores:\n",
      "-1\t0.594735760383695\tBernoulliNB(input_matrix, BernoulliNB__alpha=0.1, BernoulliNB__fit_prior=False)\n",
      "-2\t0.6820957606499649\tGaussianNB(XGBClassifier(input_matrix, XGBClassifier__learning_rate=0.001, XGBClassifier__max_depth=10, XGBClassifier__min_child_weight=4, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.7500000000000001))\n",
      "-3\t0.6914488245847021\tGaussianNB(PolynomialFeatures(XGBClassifier(input_matrix, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=2, XGBClassifier__min_child_weight=2, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.35000000000000003), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False))\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 95\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 3 - Current Pareto front scores:\n",
      "-1\t0.594735760383695\tBernoulliNB(input_matrix, BernoulliNB__alpha=0.1, BernoulliNB__fit_prior=False)\n",
      "-2\t0.6820957606499649\tGaussianNB(XGBClassifier(input_matrix, XGBClassifier__learning_rate=0.001, XGBClassifier__max_depth=10, XGBClassifier__min_child_weight=4, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.7500000000000001))\n",
      "-3\t0.6915233801635562\tBernoulliNB(FastICA(MaxAbsScaler(input_matrix), FastICA__tol=0.55), BernoulliNB__alpha=1.0, BernoulliNB__fit_prior=False)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 cosine was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Generation 4 - Current Pareto front scores:\n",
      "-1\t0.594735760383695\tBernoulliNB(input_matrix, BernoulliNB__alpha=0.1, BernoulliNB__fit_prior=False)\n",
      "-2\t0.6978170026660276\tGaussianNB(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=8, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100))\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 68\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances.\n",
      "Generation 5 - Current Pareto front scores:\n",
      "-1\t0.594735760383695\tBernoulliNB(input_matrix, BernoulliNB__alpha=0.1, BernoulliNB__fit_prior=False)\n",
      "-2\t0.7006090092428947\tGaussianNB(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=8, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100))\n",
      "-3\t0.70947687944962\tBernoulliNB(FastICA(CombineDFs(input_matrix, LogisticRegression(input_matrix, LogisticRegression__C=5.0, LogisticRegression__dual=True, LogisticRegression__penalty=l2)), FastICA__tol=1.0), BernoulliNB__alpha=1.0, BernoulliNB__fit_prior=False)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 78\n",
      "\n",
      "\n",
      "TPOT closed during evaluation in one generation.\n",
      "WARNING: TPOT may not provide a good pipeline if TPOT is stopped/interrupted in a early generation.\n",
      "\n",
      "\n",
      "TPOT closed prematurely. Will use the current best pipeline.\n",
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TPOTClassifier(config_dict=None, crossover_rate=0.1, cv=5,\n",
       "        disable_update_check=False, early_stop=None, generations=100,\n",
       "        max_eval_time_mins=5, max_time_mins=None, memory=None,\n",
       "        mutation_rate=0.9, n_jobs=1, offspring_size=None,\n",
       "        periodic_checkpoint_folder=None, population_size=100,\n",
       "        random_state=None, scoring=<function robin_metric at 0x11ce5fea0>,\n",
       "        subsample=1.0, use_dask=False, verbosity=3, warm_start=False)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpot = TPOTClassifier(verbosity=3, scoring=robin_metric)\n",
    "tpot.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate the Pareto Front pipelines manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BernoulliNB(input_matrix, BernoulliNB__alpha=0.1, BernoulliNB__fit_prior=False)': Pipeline(memory=None,\n",
       "      steps=[('bernoullinb', BernoulliNB(alpha=0.1, binarize=0.0, class_prior=None, fit_prior=False))]),\n",
       " 'BernoulliNB(FastICA(input_matrix, FastICA__tol=0.05), BernoulliNB__alpha=0.001, BernoulliNB__fit_prior=False)': Pipeline(memory=None,\n",
       "      steps=[('fastica', FastICA(algorithm='parallel', fun='logcosh', fun_args=None, max_iter=200,\n",
       "     n_components=None, random_state=None, tol=0.05, w_init=None,\n",
       "     whiten=True)), ('bernoullinb', BernoulliNB(alpha=0.001, binarize=0.0, class_prior=None, fit_prior=False))])}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpot.pareto_front_fitted_pipelines_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = tpot.pareto_front_fitted_pipelines_['GaussianNB(PCA(input_matrix, PCA__iterated_power=6, PCA__svd_solver=randomized))']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8604651162790697"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7490566037735849"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "robin_metric(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
